---
title: "Lab 2: Data Wrangling in R"
author: "Brian Cervantes Alvarez"
date: today
date-format: long
format: 
    OSUstyle-html:
        toc: true
        toc-location: right
execute: 
  echo: true
  warning: false
webr:
  packages: ['readr', 'dplyr', 'lubridate', 'stringr', 'tidyr'] # Install R packages on document open
  show-startup-message: false    # Disable displaying status of webR initialization
filters:
- webr
---

## Learning Objectives

1. Understand the importance of data wrangling in the data analysis process.
2. Transform and map raw data into a more suitable format for analysis.
3. Emphasize the significance of data cleaning and preparation.
4. Ensure data accuracy and consistency.
5. Make reliable and valid conclusions based on well-prepared data.

## What is Data Wrangling?

Data wrangling is the process of converting messy, untidy data into a tidy format, making it suitable for data visualization and analysis.

- **Data is often messy:** Real-world data is rarely provided in a tidy format.
- **Industry challenges:** Many industries have poorly designed data structures, requiring data preparation before visualization.
- **Rarely tidy datasets:** It is uncommon to receive a dataset that is already tidy.

## What Causes Untidy Data?

- **Incorrect/Inconsistent dates:** Dates can often be a headache due to inconsistencies.
- **Wide format times:** Time data is often given in a wide format instead of long.
- **Void or misspelled descriptions:** Descriptions can have gaps or missing/misspelled characters.
- **Missing values:** Many datasets have missing values in some rows or columns.
- **Condensed or incorrect headers:** Column names are often too short or mislabeled.
- **Row content split:** Sometimes, data in a single row needs to be divided into multiple columns.

Mastering data wrangling is crucial because you might have to handle datasets with millions of rows and hundreds of columns. 

In most cases, you will import data using SQL to create narrower datasets. However, when given large datasets, you can use R to manipulate and create subset datasets for focused analysis.

## Tidy Data

Tidy data is a structured format that aligns the organization of a dataset with its underlying meaning. In tidy data:

- **Each variable has its own column:** Every column in the dataset corresponds to a specific variable or attribute.
- **Each observation has its own row:** Every row captures a single observation or data entry.
- **Each cell contains a single value:** Each cell holds one distinct piece of information for a particular variable and observation.

Creating a large dataset that can be used across multiple examples requires a dataset with a variety of columns and data types. Below is a script to generate a synthetic dataset in R that can be used for filtering, selecting, mutating, summarizing, and joining examples.

## Data wrangling 101

First, ensure you have the necessary packages installed and loaded. We will use the `dplyr`, `lubridate`, `readr`, `stringr`, and `tidyr` packages for our examples.
```{r}
library(dplyr)
library(lubridate)
library(stringr)
library(tidyr)
library(readr)
```

### Download the Data

```{webr-r}
#| autorun: true
# Specify the data URL using HTTPS
url1 <- "https://howarder.github.io/ST_437_Data_Viz/Datasets/countries.csv"
url2 <- "https://howarder.github.io/ST_437_Data_Viz/Datasets/countriesExtraInfo.csv"

# Download the data files from the HTTPS URL and save it as
# countries.csv & countriesExtraInfo.csv
cat("Downloading the data ...\n")
download.file(url1, "countries.csv")
download.file(url2, "countriesExtraInfo.csv")

# Check for the data.
cat("After downloading the data, we now have:\n")
list.files()

# Read the countries data into R
countriesDs <- read_csv("countries.csv", show_col_types = FALSE)
countriesExtraDs <- read_csv("countriesExtraInfo.csv", show_col_types = FALSE)
```

### Verify Dataset with `head()`

```{webr-r}
head(countriesDs, 5)
head(countriesExtraDs, 5)
```

#### Description of the Dataset
- **`id`**: Unique identifier for each row.
- **`country`**: Country name from a predefined list.
- **`year`**: Years between 2000 and 2023.
- **`population`**: Population size between 1 million and 1.5 billion.
- **`cases`**: Number of cases where electrical services were interrupted.
- **`gdp_per_capita`**: GDP per capita.
- **`date`**: Dates from 2000 and 2023.
- **`temperature`**: Temperature in Celsius.
- **`region`**: Region of the world

#### Additional Dataset
- **`country`**: Matches the country names from the main dataset.
- **`continent`**: Continent corresponding to each country.
- **`avg_life_expectancy`**: Average life expectancy.

### Important Tidyverse Functions
- **Filtering**: Filter data based on conditions such as year, country, or region.
- **Selecting**: Select specific columns for focused analysis.
- **Mutating**: Create new columns, such as cases per 100,000 population.
- **Summarizing**: Aggregate data by country, year, or region to find totals and averages.
- **Joining**: Combine the main dataset with the additional data based on country.

### Diving into Data wrangling

#### Filtering: 
Filter data by year, country, or region.
```{webr-r}
filteredData <- countriesDs %>%
  filter(year >= 2010 & region == "Asia")

head(filteredData, 5)
```

##### Filtering Challenges:
1. **Filter the dataset** to include only countries from Africa where the population exceeds 50 million.
2. **Filter data** to show only the years 2015-2020 for countries in Europe with GDP per capita above $30,000.
3. **Create a subset** of the data for countries in the Americas with more than 100,000 cases and display the first 10 rows.
4. **Identify the countries** in Asia where the average temperature is below 10Â°C between 2010 and 2020.
5. **Filter out** any entries with missing values in the `gdp_per_capita` column for the years 2000-2010.


#### Selecting: 
Select specific columns for analysis.
```{webr-r}
selectedData <- filteredData %>%
  select(country, year, cases, population)

head(selectedData, 5)
```

##### Selecting Challenges:
1. **Select columns** related to economic indicators (e.g., `country`, `gdp_per_capita`, `population`) for further analysis.
2. **Create a dataset** with only the `year`, `cases`, and `temperature` columns for all countries and show the first 5 rows.
3. **Choose columns** that exclude any geographical information and check the first 10 rows.
4. **Select and rename** the `country` and `population` columns to `nation` and `pop_size`, respectively.
5. **Create a new dataset** with only the `year`, `population`, and a newly created column, `population_in_millions` (which should be calculated as `population / 1e6`).


#### Mutating: 
Create new columns, such as cases per 100,000 population.
```{webr-r}
mutatedData <- countriesDs %>%
  mutate(cases_per_100k = cases / population * 100000)

head(mutatedData, 5)
```


##### Mutating Challenges:
1. **Create a new column** called `gdp_total` that multiplies `gdp_per_capita` by `population`.
2. **Generate a column** that calculates cases per capita by dividing `cases` by `population` and multiplying by 100,000.
3. **Add a new column** that indicates whether a country's GDP per capita is above or below the global average (use a logical indicator).
4. **Create a `cases_per_million` column** by dividing `cases` by the population and multiplying by 1,000,000.
5. **Mutate the `date` column** to create a new column, `year_month`, that extracts the year and month information.


#### Summarizing: 
Aggregate data by country, year, or region.
```{webr-r}
summaryData <- countriesDs %>%
  group_by(country) %>%
  summarize(
    total_cases = sum(cases),
    avg_gdp_per_capita = mean(gdp_per_capita, na.rm = TRUE)
  )

head(summaryData, 5)
```

##### Summarizing Challenges:
1. **Summarize the dataset** by finding the average temperature for each region.
2. **Aggregate the data** to find the total population and total cases for each continent.
3. **Group the data** by country and summarize to find the maximum and minimum GDP per capita for each country.
4. **Summarize by year** to find the total number of cases and the average population size each year.
5. **Create a summary** that calculates the average life expectancy and total population for countries with a GDP per capita above $20,000.


#### Joining: 
Combine main dataset with additional data based on the country.
```{webr-r}
joinedData <- left_join(countriesDs, countriesExtraDs, by = "country")

head(joinedData, 5)
```


##### Joining Challenges:
1. **Perform a left join** on the main dataset and an additional dataset that contains information on urbanization rates by country.
2. **Join the datasets** to add `avg_life_expectancy` and then filter for countries where life expectancy is below 60 years.
3. **Create a new dataset** by performing an inner join on `countriesDs` and `countriesExtraDs`, then analyze the countries present in both datasets.
4. **Perform a full join** to combine the datasets and identify countries that are missing from one dataset but present in the other.
5. **Use an anti-join** to find the countries in the main dataset that do not have corresponding information in the additional dataset.

#### Combining all the steps
```{webr-r}
# Combining the previous steps into one
wrangledData <- countriesDs |>
  filter(year >= 2010 & region == "Asia") |>
  select(country, year, cases, population, gdp_per_capita) |>
  mutate(cases_per_100k = cases / population * 100000) |>
  group_by(country) |>
  summarize(
    total_cases = sum(cases),
    avg_cases_per_100k = mean(cases_per_100k),
    avg_gdp_per_capita = mean(gdp_per_capita)
  )

# Print the first few rows of the datasets
head(data, 5)
head(wrangledData, 5)

mean(data$cases)
min(data$cases)
max(data$cases)

```





