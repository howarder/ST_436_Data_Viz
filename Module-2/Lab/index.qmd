---
title: "Lab 2: Data Wrangling in R"
author: "Brian Cervantes Alvarez"
date: today
date-format: long
format: 
    OSUstyle-html:
        toc: true
        toc-location: right
execute: 
  echo: true
  warning: false
webr:
  packages: ['ggplot2', 'readr', 'dplyr', 'lubridate', 'tidyr'] # Install R packages on document open
  show-startup-message: false    # Disable displaying status of webR initialization
filters:
- webr
---

## Learning Objectives

1. Understand the importance of data wrangling in the data analysis process.
2. Transform and map raw data into a more suitable format for analysis.
3. Emphasize the significance of data cleaning and preparation.
4. Ensure data accuracy and consistency.
5. Make reliable and valid conclusions based on well-prepared data.

## What is Data Wrangling?

Data wrangling is the process of converting messy, untidy data into a tidy format, making it suitable for data visualization and analysis.

- **Data is often messy:** Real-world data is rarely provided in a tidy format.
- **Industry challenges:** Many industries have poorly designed data structures, requiring data preparation before visualization.
- **Rarely tidy datasets:** It is uncommon to receive a dataset that is already tidy.

## What Causes Untidy Data?

- **Incorrect/Inconsistent dates:** Dates can often be a headache due to inconsistencies.
- **Wide format times:** Time data is often given in a wide format instead of long.
- **Void or misspelled descriptions:** Descriptions can have gaps or missing/misspelled characters.
- **Missing values:** Many datasets have missing values in some rows or columns.
- **Condensed or incorrect headers:** Column names are often too short or mislabeled.
- **Row content split:** Sometimes, data in a single row needs to be divided into multiple columns.

Mastering data wrangling is crucial because you might have to handle datasets with millions of rows and hundreds of columns. 

In most cases, you will import data using SQL to create narrower datasets. However, when given large datasets, you can use R to manipulate and create subset datasets for focused analysis.

## Tidy Data

Tidy data is a structured format that aligns the organization of a dataset with its underlying meaning. In tidy data:

- **Each variable has its own column:** Every column in the dataset corresponds to a specific variable or attribute.
- **Each observation has its own row:** Every row captures a single observation or data entry.
- **Each cell contains a single value:** Each cell holds one distinct piece of information for a particular variable and observation.

Creating a large dataset that can be used across multiple examples requires a dataset with a variety of columns and data types. Below is a script to generate a synthetic dataset in R that can be used for filtering, selecting, mutating, summarizing, and joining examples.

## Data wrangling 101

First, ensure you have the necessary packages installed and loaded. We will use the `dplyr`, `lubridate`, `readr`, `stringr`, and `tidyr` packages for our examples.
```{r}
library(ggplot2)
library(dplyr)
library(lubridate)
library(tidyr)
library(readr)
```

### Download the Data

```{webr-r}
#| autorun: true
# Specify the data URL using HTTPS
url1 <- "https://howarder.github.io/ST_437_Data_Viz/Datasets/countries.csv"
url2 <- "https://howarder.github.io/ST_437_Data_Viz/Datasets/countriesExtraInfo.csv"

# Download the data files from the HTTPS URL and save it as
# countries.csv & countriesExtraInfo.csv
cat("Downloading the data ...\n")
download.file(url1, "countries.csv")
download.file(url2, "countriesExtraInfo.csv")

# Check for the data.
cat("After downloading the data, we now have:\n")
list.files()

# Read the countries data into R
countriesDs <- read_csv("countries.csv", show_col_types = FALSE)
countriesExtraDs <- read_csv("countriesExtraInfo.csv", show_col_types = FALSE)
```

### Verify Dataset with `head()`

```{webr-r}
head(countriesDs, 5)
head(countriesExtraDs, 5)
```


#### Description of the Dataset
- **`id`**: Unique identifier for each row.
- **`country`**: Country name from a predefined list.
- **`year`**: Years between 2000 and 2023.
- **`population`**: Population size between 1 million and 1.5 billion.
- **`cases`**: Number of cases where electrical services were interrupted.
- **`gdp_per_capita`**: GDP per capita.
- **`date`**: Dates from 2000 and 2023.
- **`temperature`**: Temperature in Celsius.
- **`region`**: Region of the world

#### Additional Dataset
- **`country`**: Matches the country names from the main dataset.
- **`continent`**: Continent corresponding to each country.
- **`avg_life_expectancy`**: Average life expectancy.

### Important Tidyverse Functions
- **Filtering**: Filter data based on conditions such as year, country, or region.
- **Selecting**: Select specific columns for focused analysis.
- **Mutating**: Create new columns, such as cases per 100,000 population.
- **Summarizing**: Aggregate data by country, year, or region to find totals and averages.
- **Joining**: Combine the main dataset with the additional data based on country.

### Diving into Data wrangling

#### Filtering 

Filtering is a powerful data wrangling tool, especially when it comes to data visualization. It helps in narrowing down the dataset to the most relevant information, making patterns easier to identify and interpret. Here are two scenarios illustrating the importance of filtering in data visualization.

#### Scenario
You are tasked with visualizing trends in the number of cases where electrical services were interrupted in Asian countries between 2010 and 2020.

#### Filtering Process
```{webr-r}
filteredData <- countriesDs %>%
  filter(year >= 2010 & year <= 2020 & region == "Asia")

head(filteredData, 5)
```

#### Visualization
```{webr-r}
ggplot(filteredData, aes(x = factor(year), y = cases, fill = country)) +
  geom_col(position = "dodge") +
  labs(title = "Electrical Service Interruptions in Asia (2010-2020)",
       x = "Year", y = "Number of Cases") +
  theme_minimal()

```

#### Why Filtering Helps

By filtering the dataset to include only Asian countries and the specific years of interest, the resulting visualization becomes more focused and easier to interpret. Instead of being overwhelmed by global data spanning 23 years, the viewer can clearly see the trends and patterns within Asia for the given period. This makes the visualization more relevant for stakeholders interested in that specific region.

#### Scenario 2:

You need to analyze the relationship between GDP per capita and life expectancy in European countries with a GDP per capita above $30,000 for the years 2015-2020.

#### Filtering Process
```{webr-r}
filteredData <- countriesDs %>%
  filter(year >= 2015 & year <= 2020 & region == "Europe" & gdp_per_capita > 30000)

filteredData <- filteredData %>%
  left_join(countriesExtraDs, by = "country")

head(filteredData, 5)
```

#### Visualization
```{webr-r}
ggplot(filteredData, aes(x = gdp_per_capita, y = avg_life_expectancy)) +
  geom_point(aes(color = country), size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "black", linetype = "dashed") +
  labs(title = "GDP per Capita vs. Life Expectancy in Europe (2015-2020)",
       x = "GDP per Capita", y = "Average Life Expectancy") +
  theme_minimal()


```

#### Why Filtering Helps

Filtering the dataset to include only countries in Europe with high GDP per capita focuses the analysis on a specific economic segment. This allows for a clearer investigation of the relationship between wealth and life expectancy, without the noise from countries with significantly different economic conditions. The visualization then highlights meaningful patterns and differences among the wealthier European countries.


##### Filtering Challenges

1. **Population Size in Africa**:
   - **Filter**: Only countries from Africa where the population exceeds 50 million.
   - **Purpose**: Isolate data for large African nations to analyze trends specific to highly populated areas.

2. **Economic Data in Europe**:
   - **Filter**: Show data only for the years 2015-2020 for European countries with GDP per capita above $30,000.
   - **Purpose**: Focus on wealthy European countries during a specific period to study economic outcomes.

3. **High-Case Countries in the Americas**:
   - **Filter**: Data for countries in the Americas with more than 100,000 cases.
   - **Purpose**: Analyze regions with significant case numbers, possibly indicating areas with frequent service interruptions.

4. **Cold Regions in Asia**:
   - **Filter**: Asian countries where the average temperature is below 10Â°C between 2010 and 2020.
   - **Purpose**: Focus on colder regions in Asia to study how temperature may correlate with service interruptions.

5. **GDP Data with Missing Values**:
   - **Filter**: Remove any entries with missing `gdp_per_capita` values for the years 2000-2010.
   - **Purpose**: Ensure clean data for economic analysis, removing incomplete records that could skew results.



#### Selecting: 
Select specific columns for analysis.
```{webr-r}
selectedData <- filteredData %>%
  select(country, year, cases, population)

head(selectedData, 5)
```

##### Selecting Challenges:
1. **Select columns** related to economic indicators (e.g., `country`, `gdp_per_capita`, `population`) for further analysis.
2. **Create a dataset** with only the `year`, `cases`, and `temperature` columns for all countries and show the first 5 rows.
3. **Choose columns** that exclude any geographical information and check the first 10 rows.
4. **Select and rename** the `country` and `population` columns to `nation` and `pop_size`, respectively.
5. **Create a new dataset** with only the `year`, `population`, and a newly created column, `population_in_millions` (which should be calculated as `population / 1e6`).


#### Mutating: 
Create new columns, such as cases per 100,000 population.
```{webr-r}
mutatedData <- countriesDs %>%
  mutate(cases_per_100k = cases / population * 100000)

head(mutatedData, 5)
```


##### Mutating Challenges:
1. **Create a new column** called `gdp_total` that multiplies `gdp_per_capita` by `population`.
2. **Generate a column** that calculates cases per capita by dividing `cases` by `population` and multiplying by 100,000.
3. **Add a new column** that indicates whether a country's GDP per capita is above or below the global average (use a logical indicator).
4. **Create a `cases_per_million` column** by dividing `cases` by the population and multiplying by 1,000,000.
5. **Mutate the `date` column** to create a new column, `year_month`, that extracts the year and month information.


#### Summarizing: 
Aggregate data by country, year, or region.
```{webr-r}
summaryData <- countriesDs %>%
  group_by(country) %>%
  summarize(
    total_cases = sum(cases),
    avg_gdp_per_capita = mean(gdp_per_capita, na.rm = TRUE)
  )

head(summaryData, 5)
```

##### Summarizing Challenges:
1. **Summarize the dataset** by finding the average temperature for each region.
2. **Aggregate the data** to find the total population and total cases for each continent.
3. **Group the data** by country and summarize to find the maximum and minimum GDP per capita for each country.
4. **Summarize by year** to find the total number of cases and the average population size each year.
5. **Create a summary** that calculates the average life expectancy and total population for countries with a GDP per capita above $20,000.


#### Joining: 
Combine main dataset with additional data based on the country.
```{webr-r}
joinedData <- left_join(countriesDs, countriesExtraDs, by = "country")

head(joinedData, 5)
```


##### Joining Challenges:
1. **Perform a left join** on the main dataset and an additional dataset that contains information on urbanization rates by country.
2. **Join the datasets** to add `avg_life_expectancy` and then filter for countries where life expectancy is below 60 years.
3. **Create a new dataset** by performing an inner join on `countriesDs` and `countriesExtraDs`, then analyze the countries present in both datasets.
4. **Perform a full join** to combine the datasets and identify countries that are missing from one dataset but present in the other.
5. **Use an anti-join** to find the countries in the main dataset that do not have corresponding information in the additional dataset.

#### Combining all the steps
```{webr-r}
# Combining the previous steps into one
wrangledData <- countriesDs |>
  filter(year >= 2010 & region == "Asia") |>
  select(country, year, cases, population, gdp_per_capita) |>
  mutate(cases_per_100k = cases / population * 100000) |>
  group_by(country) |>
  summarize(
    total_cases = sum(cases),
    avg_cases_per_100k = mean(cases_per_100k),
    avg_gdp_per_capita = mean(gdp_per_capita)
  )

# Print the first few rows of the datasets
head(data, 5)
head(wrangledData, 5)

mean(data$cases)
min(data$cases)
max(data$cases)

```





